{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c6cdb0c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-09T09:02:43.375097Z",
     "iopub.status.busy": "2025-10-09T09:02:43.374565Z",
     "iopub.status.idle": "2025-10-09T09:02:47.429617Z",
     "shell.execute_reply": "2025-10-09T09:02:47.429018Z"
    },
    "papermill": {
     "duration": 4.059563,
     "end_time": "2025-10-09T09:02:47.431034",
     "exception": false,
     "start_time": "2025-10-09T09:02:43.371471",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "288639da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-09T09:02:47.435611Z",
     "iopub.status.busy": "2025-10-09T09:02:47.435322Z",
     "iopub.status.idle": "2025-10-09T09:02:47.445624Z",
     "shell.execute_reply": "2025-10-09T09:02:47.445045Z"
    },
    "papermill": {
     "duration": 0.013708,
     "end_time": "2025-10-09T09:02:47.446704",
     "exception": false,
     "start_time": "2025-10-09T09:02:47.432996",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        \n",
    "        #LAYER 1 (3 Neurons) \n",
    "        self.w00 = torch.tensor(0.50, requires_grad=True)\n",
    "        self.b00 = torch.tensor(0.10, requires_grad=True)\n",
    "        self.w01 = torch.tensor(-1.20, requires_grad=True)\n",
    "        self.b01 = torch.tensor(0.00, requires_grad=True)\n",
    "        self.w02 = torch.tensor(0.70, requires_grad=True)\n",
    "        self.b02 = torch.tensor(-0.30, requires_grad=True)\n",
    "        \n",
    "        #  LAYER 2 (2 Neurons)\n",
    "        self.w10 = torch.tensor([0.40, -0.60, 0.20], requires_grad=True)  # for neuron 0\n",
    "        self.b10 = torch.tensor(0.05, requires_grad=True)\n",
    "        self.w11 = torch.tensor([-0.30, 0.80, -0.50], requires_grad=True) # for neuron 1\n",
    "        self.b11 = torch.tensor(0.00, requires_grad=True)\n",
    "        \n",
    "        # OUTPUT LAYER \n",
    "        self.w20 = torch.tensor(1.50, requires_grad=True)\n",
    "        self.b20 = torch.tensor(-0.40, requires_grad=True)\n",
    "        \n",
    "        # initial weights and biases\n",
    "        print(\"Initial Parameters:\")\n",
    "        print(\"- Layer 1:\")\n",
    "        print(f\"  N0: w={self.w00.item()}, b={self.b00.item()}\")\n",
    "        print(f\"  N1: w={self.w01.item()}, b={self.b01.item()}\")\n",
    "        print(f\"  N2: w={self.w02.item()}, b={self.b02.item()}\")\n",
    "        print(\"- Layer 2:\")\n",
    "        print(f\"  N0: w={self.w10.tolist()}, b={self.b10.item()}\")\n",
    "        print(f\"  N1: w={self.w11.tolist()}, b={self.b11.item()}\")\n",
    "        print(\"- Output Layer:\")\n",
    "        print(f\"  N0: w={self.w20.item()}, b={self.b20.item()}\")\n",
    "     \n",
    "\n",
    "    def forward(self, x):\n",
    "        print(\"\\n FORWARD PROPAGATION \")\n",
    "        \n",
    "        #  LAYER 1 \n",
    "        print(\"\\n[Layer 1: ReLU Activation]\")\n",
    "        \n",
    "        z00 = self.w00 * x + self.b00\n",
    "        a00 = torch.relu(z00)\n",
    "        print(f\"Neuron 0: {self.w00.item():.2f}*{x.item():.2f}+{self.b00.item():.2f} = {z00.item():.3f}  | ReLU = {a00.item():.3f}\")\n",
    "        \n",
    "        z01 = self.w01 * x + self.b01\n",
    "        a01 = torch.relu(z01)\n",
    "        print(f\"Neuron 1: {self.w01.item():.2f}*{x.item():.2f}+{self.b01.item():.2f} = {z01.item():.3f}  | ReLU = {a01.item():.3f}\")\n",
    "        \n",
    "        z02 = self.w02 * x + self.b02\n",
    "        a02 = torch.relu(z02)\n",
    "        print(f\"Neuron 2: {self.w02.item():.2f}*{x.item():.2f}+{self.b02.item():.2f} = {z02.item():.3f}  | ReLU = {a02.item():.3f}\")\n",
    "        \n",
    "        act_l1 = torch.cat((a00.unsqueeze(0), a01.unsqueeze(0), a02.unsqueeze(0)))\n",
    "        \n",
    "        #  LAYER 2 \n",
    "        print(\"\\n[Layer 2: Sigmoid Activation]\")\n",
    "        \n",
    "        z10 = torch.dot(self.w10, act_l1) + self.b10\n",
    "        a10 = torch.sigmoid(z10)\n",
    "        print(f\"Neuron 0: dot(w10, a1)+b10 = {z10.item():.3f} | Sigmoid = {a10.item():.3f}\")\n",
    "        \n",
    "        z11 = torch.dot(self.w11, act_l1) + self.b11\n",
    "        a11 = torch.sigmoid(z11)\n",
    "        print(f\"Neuron 1: dot(w11, a1)+b11 = {z11.item():.3f} | Sigmoid = {a11.item():.3f}\")\n",
    "        \n",
    "        #  TANH COMBINATION \n",
    "        print(\"\\n[Combine & Tanh Activation]\")\n",
    "        summed = a10 + a11\n",
    "        tanh_out = torch.tanh(summed)\n",
    "        print(f\"Combine: {a10.item():.3f} + {a11.item():.3f} = {summed.item():.3f}\")\n",
    "        print(f\"Tanh({summed.item():.3f}) = {tanh_out.item():.3f}\")\n",
    "        \n",
    "        # OUTPUT LAYER \n",
    "        print(\"\\n[Output Layer: Linear Activation]\")\n",
    "        final_out = self.w20 * tanh_out + self.b20\n",
    "        print(f\"Output: {self.w20.item():.2f}*{tanh_out.item():.3f}+{self.b20.item():.2f} = {final_out.item():.3f}\")\n",
    "        \n",
    "        \n",
    "        print(\"FORWARD PROPAGATION DONE\")\n",
    "        return final_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a474272",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-09T09:02:47.450153Z",
     "iopub.status.busy": "2025-10-09T09:02:47.449953Z",
     "iopub.status.idle": "2025-10-09T09:02:47.549073Z",
     "shell.execute_reply": "2025-10-09T09:02:47.548305Z"
    },
    "papermill": {
     "duration": 0.102475,
     "end_time": "2025-10-09T09:02:47.550600",
     "exception": false,
     "start_time": "2025-10-09T09:02:47.448125",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL  \n",
      "Initial Parameters:\n",
      "- Layer 1:\n",
      "  N0: w=0.5, b=0.10000000149011612\n",
      "  N1: w=-1.2000000476837158, b=0.0\n",
      "  N2: w=0.699999988079071, b=-0.30000001192092896\n",
      "- Layer 2:\n",
      "  N0: w=[0.4000000059604645, -0.6000000238418579, 0.20000000298023224], b=0.05000000074505806\n",
      "  N1: w=[-0.30000001192092896, 0.800000011920929, -0.5], b=0.0\n",
      "- Output Layer:\n",
      "  N0: w=1.5, b=-0.4000000059604645\n",
      "\n",
      "Input: x = 1.25, requires_grad = True\n",
      "\n",
      " FORWARD PROPAGATION \n",
      "\n",
      "[Layer 1: ReLU Activation]\n",
      "Neuron 0: 0.50*1.25+0.10 = 0.725  | ReLU = 0.725\n",
      "Neuron 1: -1.20*1.25+0.00 = -1.500  | ReLU = 0.000\n",
      "Neuron 2: 0.70*1.25+-0.30 = 0.575  | ReLU = 0.575\n",
      "\n",
      "[Layer 2: Sigmoid Activation]\n",
      "Neuron 0: dot(w10, a1)+b10 = 0.455 | Sigmoid = 0.612\n",
      "Neuron 1: dot(w11, a1)+b11 = -0.505 | Sigmoid = 0.376\n",
      "\n",
      "[Combine & Tanh Activation]\n",
      "Combine: 0.612 + 0.376 = 0.988\n",
      "Tanh(0.988) = 0.757\n",
      "\n",
      "[Output Layer: Linear Activation]\n",
      "Output: 1.50*0.757+-0.40 = 0.735\n",
      "FORWARD PROPAGATION DONE\n",
      "\n",
      "FINAL OUTPUT VALUE: 0.734887\n"
     ]
    }
   ],
   "source": [
    "# MODEL \n",
    "print(\"MODEL  \")\n",
    "\n",
    "model = NeuralNet()\n",
    "\n",
    "x = torch.tensor(1.25, requires_grad=True)\n",
    "print(f\"\\nInput: x = {x.item()}, requires_grad = {x.requires_grad}\")\n",
    "\n",
    "#FORWARD PASS \n",
    "output = model(x)\n",
    "print(f\"\\nFINAL OUTPUT VALUE: {output.item():.6f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61a646aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-09T09:02:47.555002Z",
     "iopub.status.busy": "2025-10-09T09:02:47.554380Z",
     "iopub.status.idle": "2025-10-09T09:02:47.670374Z",
     "shell.execute_reply": "2025-10-09T09:02:47.669709Z"
    },
    "papermill": {
     "duration": 0.11919,
     "end_time": "2025-10-09T09:02:47.671467",
     "exception": false,
     "start_time": "2025-10-09T09:02:47.552277",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BACKWARD PASS\n",
      ">>> GRADIENTS <<<\n",
      "dOutput/dx: -0.023479\n",
      "w00.grad: 0.019708\n",
      "b00.grad: 0.015767\n",
      "w10.grad: [0.11043081432580948, 0.0, 0.08758305758237839]\n",
      "b10.grad: 0.152318\n",
      "w20.grad: 0.756591\n",
      "b20.grad: 1.000000\n"
     ]
    }
   ],
   "source": [
    "#  BACKPROPAGATION\n",
    "print(\"\\nBACKWARD PASS\")\n",
    "\n",
    "params = [model.w00, model.b00, model.w01, model.b01, model.w02, model.b02,\n",
    "          model.w10, model.b10, model.w11, model.b11, model.w20, model.b20]\n",
    "for p in params:\n",
    "    if p.grad is not None:\n",
    "        p.grad.zero_()\n",
    "if x.grad is not None:\n",
    "    x.grad.zero_()\n",
    "\n",
    "# Backward\n",
    "output.backward()\n",
    "\n",
    "print(\">>> GRADIENTS <<<\")\n",
    "print(f\"dOutput/dx: {x.grad.item():.6f}\")\n",
    "print(f\"w00.grad: {model.w00.grad.item():.6f}\")\n",
    "print(f\"b00.grad: {model.b00.grad.item():.6f}\")\n",
    "print(f\"w10.grad: {model.w10.grad.tolist()}\")\n",
    "print(f\"b10.grad: {model.b10.grad.item():.6f}\")\n",
    "print(f\"w20.grad: {model.w20.grad.item():.6f}\")\n",
    "print(f\"b20.grad: {model.b20.grad.item():.6f}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31154,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 9.052001,
   "end_time": "2025-10-09T09:02:48.692338",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-10-09T09:02:39.640337",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
